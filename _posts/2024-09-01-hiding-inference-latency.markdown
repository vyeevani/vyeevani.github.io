---
layout: post
title:  "Hiding Action Prediction Inference Time in Robotics Applications"
date:   2024-09-16 14:00:00 -0700
categories: Robotics
usemathjax: true
published: true
---
Reducing inference latency in robotics is critical to smooth robotics. To be explicitly clear, I'm talking about worst-case per action inference latency.

The strategy that generally has been persued is action chunking. This strategy leverages predicting the next few actions from a specific observation. This clearly would reduce the average action inference latency by reducing the number of inferences that you are doing. But every few timesteps, you will still have to do an inference which would take much longer. This results in a choppy execution pattern where you move for a longer period of time. There's a simple enough trick here using pipelining. At training time for a sequence model that takes in a sequence of observations and predicts a sequence of actions, you would predict the actions associated with the next observation as well as the current observation. There's a critical problem with this approach that stems from control theory. Simple analogy to represent the problem: if you were asked to close your eyes and walk in a straight line, odds are you can't do it. In other words, without some information about the true state of the world, predicting too far into the future becomes a problem.

Recently, there was a paper titled [Diffusion Forcing](https://arxiv.org/pdf/2407.01392). I'll briefly sketch what the paper discusses. It explores how diffusion can be applied to sequence modeling problems by using a per-sequence diffusion level. For instance, if you had a world model and wanted to use diffusion forcing, you would noise the observations with different diffusion levels for each timestep in your world model. This approach contrasts with methods like [Video Diffusion Models](https://arxiv.org/pdf/2204.03458). Side note, I've tried video diffusion models for robotics applications. It worked but tuning it was a huge pain. I had to carefully assign learning rates to the actions and observations independently. Also deploying gradient descent methods on edge hardware isn't trivial given most edge hardware explicitly lack the kernels for gradient operations. Hardware makers don't want you to train on their edge stuff so they disallow ops used for training.

In diffusion forcing, authors take the forward diffusion process in DDPM style training as a "partial masking" in the style of casual masking from transformers. With this perspective you can explicitly represent your uncertainty about the future. Imagine that you have a trajectory model that predicts observations and actions now. You trained it using diffusion forcing on a trajectory dataset. You could now start predicting the future actions while holding the noise levels of the observations steady. You can start denoising the actions a bit, then when the observations are ready, you can then assign the observations some value close to certainty (associated with diffusion steps close to 0). Then you complete the rollout of the action. While you do this rollout, you also start the partial rollout of the future actions. Therefore, you can reduce the number of diffusion steps you are taking for a given action at the time of prediction. Instead you are running the background process of constantly predicting the action which hides the latency while not discounting the fact that you don't have all the information required to make the prediction and that you will get that information at some point. 

I also find this approach highly appealing because it gives you a really natural way of down grading your model's predictive power. For example, if you had a much weaker target hardware. You could dynamically increase the number of timesteps that you are predicting into the future. This would result in worse quality predictions but allows you to just train one single model. Of course, this applies as far as your memory limit will allow.